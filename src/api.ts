import { Message, StreamingCallback, ModelResponse, UsageData } from './types';

// Define an interface for errors that should not be retried
interface NoRetryError extends Error {
  noRetry?: boolean;
}

// Helper function to retry a function with exponential backoff
async function withRetry<T>(
  fn: () => Promise<T>,
  maxRetries: number = 2,
  initialDelay: number = 500
): Promise<T> {
  let retries = 0;
  let lastError: Error;

  while (retries <= maxRetries) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;

      // Don't retry if this is an abort error (request was cancelled)
      // or if the error has the noRetry flag set
      if (
        (error instanceof DOMException && error.name === 'AbortError') ||
        (error as NoRetryError).noRetry === true
      ) {
        console.log('Not retrying due to error type or noRetry flag');
        throw error;
      }

      // If we've exhausted our retries, throw the error
      if (retries === maxRetries) {
        console.error(`Failed after ${retries + 1} attempts:`, error);
        throw error;
      }

      // Calculate delay with exponential backoff (500ms, 1000ms, etc.)
      const delay = initialDelay * Math.pow(2, retries);
      console.log(`Attempt ${retries + 1} failed, retrying in ${delay}ms...`);

      // Wait before retrying
      await new Promise((resolve) => setTimeout(resolve, delay));
      retries++;
    }
  }

  // This should never be reached due to the throw in the loop, but TypeScript needs it
  throw lastError!;
}

// Helper function to process streaming responses
async function processStream(
  reader: ReadableStreamDefaultReader<Uint8Array>,
  onChunk: StreamingCallback
): Promise<{ content: string; usage?: UsageData; generationId?: string }> {
  const decoder = new TextDecoder();
  let fullText = '';
  let usage: UsageData | undefined;
  let generationId: string | undefined;

  try {
    while (true) {
      const { done, value } = await reader.read();

      if (done) {
        // Signal completion
        onChunk('', true);
        break;
      }

      // Decode the chunk
      const chunk = decoder.decode(value, { stream: true });

      // Process the chunk (handle SSE format)
      const lines = chunk.split('\n');
      for (const line of lines) {
        if (line.startsWith('data: ') && line !== 'data: [DONE]') {
          try {
            const data = JSON.parse(line.substring(6));
            let content = '';

            // Extract content based on response format
            if (data.choices && data.choices[0]) {
              if (data.choices[0].text) {
                // Hyperbolic completion format
                content = data.choices[0].text;
              } else if (
                data.choices[0].delta &&
                data.choices[0].delta.content
              ) {
                // OpenRouter streaming format (newer API versions)
                content = data.choices[0].delta.content;
              } else if (
                data.choices[0].message &&
                data.choices[0].message.content
              ) {
                // OpenRouter format (older API versions)
                content = data.choices[0].message.content;
              }
            }

            // Extract usage data if present (usually in the final chunk)
            if (data.usage) {
              usage = {
                promptTokens: data.usage.prompt_tokens || 0,
                completionTokens: data.usage.completion_tokens || 0,
                totalTokens: data.usage.total_tokens || 0,
              };
            }

            // Extract generation ID if present (OpenRouter)
            if (data.id) {
              generationId = data.id;
            }

            if (content) {
              // handle initial whitespace generated by hyperbolic completion
              if (fullText === '') {
                content = content.trimStart();
              }
              fullText += content;
              onChunk(content, false);
            }
            // ignore chunks without content
          } catch (e) {
            console.error('Error parsing SSE data:', e);
          }
        }
      }
    }
  } catch (error) {
    // Check if this is an abort error (request was cancelled)
    if (error instanceof DOMException && error.name === 'AbortError') {
      console.log('Stream reading was cancelled');
      throw new Error('Request cancelled');
    } else {
      console.error('Error reading stream:', error);
      throw error;
    }
  }

  return { content: fullText, usage, generationId };
}

export async function openrouterConversation(
  actor: string,
  model: string,
  context: Message[],
  systemPrompt: string | null,
  openrouterKey: string,
  maxTokens: number = 1024,
  onChunk?: StreamingCallback,
  abortSignal?: AbortSignal,
  seed?: number
): Promise<ModelResponse> {
  const messages = context.map((m) => ({ role: m.role, content: m.content }));

  // Add system prompt if provided
  if (systemPrompt) {
    messages.unshift({ role: 'system', content: systemPrompt });
  }

  const requestBody: any = {
    model,
    messages,
    temperature: 1.0,
    max_tokens: maxTokens,
    stream: true,
    usage: { include: true }, // Request usage data in response
  };

  // Add seed if provided
  if (seed !== undefined) {
    requestBody.seed = seed;
  }

  // Create a flag to track if we've started receiving a response
  let hasStartedReceivingResponse = false;

  // Create a wrapper for onChunk that sets the flag when we receive content
  const onChunkWrapper: StreamingCallback | undefined = onChunk
    ? (chunk: string, isDone: boolean) => {
        if (chunk && !isDone) {
          hasStartedReceivingResponse = true;
        }
        onChunk(chunk, isDone);
      }
    : undefined;

  return withRetry(async () => {
    try {
      const response = await fetch(
        'https://openrouter.ai/api/v1/chat/completions',
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            Authorization: `Bearer ${openrouterKey}`,
            'HTTP-Referer': window.location.origin,
            'X-Title': 'backrooms.directory',
          },
          body: JSON.stringify(requestBody),
          signal: abortSignal,
        }
      );

      if (!response.ok) {
        throw new Error(
          `OpenRouter API error: ${response.status} ${response.statusText}`
        );
      }

      // Process the stream
      if (onChunkWrapper && response.body) {
        const reader = response.body.getReader();
        const result = await processStream(reader, onChunkWrapper);
        return {
          content: result.content,
          usage: result.usage || {
            promptTokens: 0,
            completionTokens: 0,
            totalTokens: 0,
          },
          generationId: result.generationId,
        };
      } else {
        // Fallback to non-streaming for backward compatibility
        const data = await response.json();
        hasStartedReceivingResponse = true;
        const usage: UsageData = data.usage
          ? {
              promptTokens: data.usage.prompt_tokens || 0,
              completionTokens: data.usage.completion_tokens || 0,
              totalTokens: data.usage.total_tokens || 0,
            }
          : { promptTokens: 0, completionTokens: 0, totalTokens: 0 };

        return {
          content: data.choices[0].message.content,
          usage,
          generationId: data.id,
        };
      }
    } catch (error) {
      // Check if this is an abort error (request was cancelled)
      if (error instanceof DOMException && error.name === 'AbortError') {
        console.log('OpenRouter API request was cancelled');
        throw new Error('Request cancelled');
      }

      // If we've already started receiving a response, don't retry
      if (hasStartedReceivingResponse) {
        console.error(
          'Error during OpenRouter API streaming (not retrying):',
          error
        );
        const errorMessage =
          error instanceof Error ? error.message : String(error);
        const noRetryError = new Error(
          `OpenRouter API error (response already started): ${errorMessage}`
        );
        (noRetryError as NoRetryError).noRetry = true;
        throw noRetryError;
      }

      console.error('Error calling OpenRouter API (will retry):', error);
      throw error;
    }
  });
}

export async function hyperbolicCompletionConversation(
  actor: string,
  model: string,
  context: Message[],
  systemPrompt: string | null,
  hyperbolicKey: string,
  maxTokens: number = 1024,
  onChunk?: StreamingCallback,
  abortSignal?: AbortSignal,
  seed?: number
): Promise<ModelResponse> {
  // Format messages into a chat-like completion prompt, as 405-base does not support chat completion
  let prompt = '';
  if (systemPrompt) {
    prompt += `System: ${systemPrompt}\n\n`;
  }

  for (const message of context.map((m) => ({
    role: m.role,
    content: m.content,
  }))) {
    prompt += `${message.role}: ${message.content}\n\n`;
  }

  prompt += 'assistant: ';

  const headers = {
    Authorization: `Bearer ${hyperbolicKey}`,
    'Content-Type': 'application/json',
  };

  const payload: any = {
    model,
    temperature: 1.0,
    max_tokens: maxTokens,
    prompt,
    stop: ['System:', 'system:', 'User:', 'Assistant:', 'user:', 'assistant:'],
    stream: true,
  };

  // Add seed if provided
  if (seed !== undefined) {
    payload.seed = seed;
  }

  // Create a flag to track if we've started receiving a response
  let hasStartedReceivingResponse = false;

  return withRetry(async () => {
    try {
      const response = await fetch(
        'https://api.hyperbolic.xyz/v1/completions',
        {
          method: 'POST',
          headers,
          body: JSON.stringify(payload),
          signal: abortSignal,
        }
      );

      if (!response.ok) {
        throw new Error(
          `Hyperbolic Completion API error: ${response.status} ${response.statusText}`
        );
      }

      // Process the stream
      if (onChunk && response.body) {
        const reader = response.body.getReader();

        // Create a wrapper for onChunk that buffers chunks and only emits them
        // when the next non-whitespace chunk is received or when the stream is done.
        //
        // Because the hyperbolic LLM is using conversation completions that end when it attempts
        // to take on a new "character", we will often see trailing newlines. We don't want to amend
        // the output after it's been emitted via the provided `onChunk`, so instead we buffer the
        // previous non-whitespace chunk and emit it with stripped newlines if it's the final chunk
        // in the completion.
        let bufferedChunk: string | null = null;

        const onChunkWrapper: StreamingCallback = (
          chunk: string,
          isDone: boolean
        ) => {
          if (isDone) {
            // If we have a buffered chunk and the stream is done,
            // emit it after stripping trailing whitespace
            if (bufferedChunk !== null) {
              onChunk(bufferedChunk.replace(/\s+$/g, ''), false);
              bufferedChunk = null;
            }
            // Signal completion
            onChunk('', true);
            return;
          }

          // If this chunk has non-whitespace content
          if (chunk.trim().length > 0) {
            // Mark that we've started receiving a response
            hasStartedReceivingResponse = true;

            // If we have a buffered chunk, emit it first
            if (bufferedChunk !== null) {
              onChunk(bufferedChunk, false);
              bufferedChunk = null;
            }

            // Buffer this chunk for next time
            bufferedChunk = chunk;
          } else if (bufferedChunk !== null) {
            // This is a whitespace-only chunk, append it to the buffer
            bufferedChunk += chunk;
          } else {
            // This is a whitespace-only chunk and we have no buffer yet
            bufferedChunk = chunk;
          }
        };

        const result = await processStream(reader, onChunkWrapper);

        // Calculate cost for Hyperbolic: $4 per 1M tokens
        const usage = result.usage || {
          promptTokens: 0,
          completionTokens: 0,
          totalTokens: 0,
        };
        const cost = (usage.totalTokens / 1000000) * 4;

        return {
          content: result.content,
          usage: { ...usage, cost },
        };
      } else {
        // Fallback to non-streaming for backward compatibility
        const data = await response.json();
        hasStartedReceivingResponse = true;

        const usage: UsageData = data.usage
          ? {
              promptTokens: data.usage.prompt_tokens || 0,
              completionTokens: data.usage.completion_tokens || 0,
              totalTokens: data.usage.total_tokens || 0,
            }
          : { promptTokens: 0, completionTokens: 0, totalTokens: 0 };

        // Calculate cost for Hyperbolic: $4 per 1M tokens
        const cost = (usage.totalTokens / 1000000) * 4;

        return {
          content: data.choices[0].text.trim(),
          usage: { ...usage, cost },
        };
      }
    } catch (error) {
      // Check if this is an abort error (request was cancelled)
      if (error instanceof DOMException && error.name === 'AbortError') {
        console.log('Hyperbolic Completion API request was cancelled');
        throw new Error('Request cancelled');
      }

      // If we've already started receiving a response, don't retry
      if (hasStartedReceivingResponse) {
        console.error(
          'Error during Hyperbolic API streaming (not retrying):',
          error
        );
        const errorMessage =
          error instanceof Error ? error.message : String(error);
        const noRetryError = new Error(
          `Hyperbolic API error (response already started): ${errorMessage}`
        );
        (noRetryError as NoRetryError).noRetry = true;
        throw noRetryError;
      }

      console.error(
        'Error calling Hyperbolic Completion API (will retry):',
        error
      );
      throw error;
    }
  });
}

// Function to fetch cost information from OpenRouter generations API
export async function getOpenRouterGenerationCost(
  generationId: string,
  openrouterKey: string
): Promise<number | null> {
  try {
    const url = `https://openrouter.ai/api/v1/generation?id=${generationId}`;

    const response = await fetch(url, {
      method: 'GET',
      headers: {
        Authorization: `Bearer ${openrouterKey}`,
        'HTTP-Referer': window.location.origin,
        'X-Title': 'backrooms.directory',
      },
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error(
        `OpenRouter Generations API error: ${response.status} ${response.statusText}`,
        errorText
      );
      return null;
    }

    const data = await response.json();
    // Return the total cost if available
    const cost = data.data.total_cost || null;
    return cost;
  } catch (error) {
    console.error('Error fetching OpenRouter generation cost:', error);
    return null;
  }
}
